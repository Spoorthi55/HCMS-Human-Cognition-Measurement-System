# Related Work

Traditional assessment systems primarily rely on correctness-based evaluation, a paradigm deeply rooted in standardized testing and automated grading systems. While efficient, such approaches fail to capture deeper cognitive properties such as reasoning stability and metacognitive awareness.

Research in educational psychology has highlighted the role of **metacognition**, particularly confidence monitoring and self-regulation, in effective learning. Studies on confidence calibration show that learners often exhibit systematic biases, including overconfidence and underconfidence, which directly impact learning outcomes. However, these insights are rarely embedded into computational assessment systems.

Intelligent Tutoring Systems (ITS) and adaptive learning platforms have attempted to personalize instruction using learner models, often based on Bayesian Knowledge Tracing or Item Response Theory. While these models estimate mastery probabilities, they typically treat confidence and consistency as secondary or implicit signals.

Recent work in learning analytics has explored behavioral features such as response time, attempt patterns, and revision frequency. Although promising, many of these approaches lack interpretability and robustness under real-world noise.

HCMS differentiates itself by explicitly modeling cognition as a **multi-signal construct**, integrating accuracy, confidence, consistency, and misconception dynamics into a unified, explainable framework. Unlike black-box models, HCMS prioritizes transparency and cognitive interpretability, aligning computational outputs with educational theory.
