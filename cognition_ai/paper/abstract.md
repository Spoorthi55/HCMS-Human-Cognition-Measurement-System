# Abstract

Correct answers are often treated as evidence of understanding. However, correctness alone cannot distinguish between stable mastery and fragile performance driven by guessing, memorization, or miscalibrated confidence. This paper argues that **static accuracy-based test scores are unreliable indicators of true understanding** because they ignore metacognitive alignment and reasoning stability.

We investigate this claim through the **Human Cognition Measurement System (HCMS)**, a cognition-aware assessment framework designed to measure understanding as a dynamic, multi-signal construct. Rather than evaluating responses solely by correctness, HCMS integrates confidence–accuracy alignment, repeated-trial consistency, and stability under controlled perturbation to infer cognitive robustness.

Across controlled experiments, we show that learners with equivalent accuracy profiles frequently diverge in cognitive stability and calibration. In particular, confidence–accuracy misalignment reliably predicts degradation in reasoning consistency under perturbation—patterns that accuracy-only metrics fail to detect. These findings demonstrate that correctness can mask unstable or miscalibrated understanding, leading to misleading assessments of mastery.

HCMS is presented not as a replacement for existing assessment models, but as an **instrument for probing cognitive validity** beyond correctness. By operationalizing metacognitive alignment and stability as measurable signals, this work provides empirical evidence that assessment systems must move beyond static scores to faithfully represent how humans understand, not just what they answer.
