# Experiments

Experiments were conducted using simulated and real learner response data across multiple programming concepts. Each learner completed multiple trials with associated confidence ratings.

The experiments aimed to evaluate:
- Detection of miscalibrated confidence
- Stability of cognitive inference
- Robustness under noisy inputs
- Interpretability of decision traces

Repeated trials were introduced to test consistency metrics. Artificial noise and adversarial confidence patterns were injected to evaluate robustness.

The system successfully differentiated between learners with similar accuracy but differing cognitive stability, demonstrating the necessity of multi-dimensional analysis.
